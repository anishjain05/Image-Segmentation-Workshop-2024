{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries\n"
      ],
      "metadata": {
        "id": "NxVDF8TcvO4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zK-N51-ZvR0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06PWXvUGN_YI"
      },
      "outputs": [],
      "source": [
        "class SegNet(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(SegNet, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        indices = []\n",
        "        sizes = []\n",
        "\n",
        "        for layer in self.encoder:\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                x, ind = layer(x)\n",
        "                indices.append(ind)\n",
        "                sizes.append(x.size())\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        for i, layer in enumerate(self.decoder):\n",
        "            if isinstance(layer, nn.MaxUnpool2d):\n",
        "                x = layer(x, indices[-(i//9)-1], output_size=sizes[-(i//9)-1])\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SegNet(num_classes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We will use the Brain Tumor Dataset, which is annotated in COCO format. The dataset class loads images and corresponding segmentation masks.\n"
      ],
      "metadata": {
        "id": "zZeYr64QUhyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJQl7FkuORe-"
      },
      "outputs": [],
      "source": [
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.ann_file = os.path.join(root_dir, '_annotations.coco.json')\n",
        "        self.coco = COCO(self.ann_file)\n",
        "        self.transform = transform\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Load image\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Create mask\n",
        "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
        "        for ann in anns:\n",
        "            mask = np.maximum(mask, self.coco.annToMask(ann))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Convert mask to tensor and add channel dimension\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "\n",
        "# Define dataset and dataloader\n",
        "root_dir = '/content/drive/MyDrive/Brain Tumor Kaggle Dataset/valid'  # Directory containing all the images and the annotations.json file\n",
        "\n",
        "dataset = BrainTumorDataset(root_dir=root_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6S_Mrc3NgtIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1l0nzNHQL3y"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training setup\n",
        "num_epochs = 25\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Results\n",
        "We will visualize some sample images and their corresponding predicted masks to see how well the model performs.\n"
      ],
      "metadata": {
        "id": "3_nnakC5U31d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display an image and its mask\n",
        "def show_sample(image, mask):\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "    ax[0].imshow(image.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
        "    ax[0].set_title('Image')\n",
        "    ax[1].imshow(mask.squeeze(0), cmap='gray')  # Remove the channel dimension\n",
        "    ax[1].set_title('Mask')\n",
        "    plt.show()\n",
        "\n",
        "# Check a sample\n",
        "data_iter = iter(dataloader)\n",
        "images, masks = next(data_iter)\n",
        "show_sample(images[0], masks[0])"
      ],
      "metadata": {
        "id": "ZNFXYihMVDRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}